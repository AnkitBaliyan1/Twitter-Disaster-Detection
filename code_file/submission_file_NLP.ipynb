{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handling file data\n",
    "import pandas as pd\n",
    "# handling numerical data\n",
    "import numpy as np\n",
    "# for ploting/visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# importing Natural Language Toolkit\n",
    "import nltk\n",
    "\n",
    "# data pre-processing\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "# transform data\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# feature reduction/\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "# model building\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# model accuracy report\n",
    "from sklearn.metrics import classification_report, confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10876, 5)"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"/Users/ankitbaliyan/Documents/VS_Code/Ongoing projects/NLP_Disaster/dataset/train.csv\")\n",
    "test = pd.read_csv(\"/Users/ankitbaliyan/Documents/VS_Code/Ongoing projects/NLP_Disaster/dataset/test.csv\")\n",
    "submission = pd.read_csv(\"//Users/ankitbaliyan/Documents/VS_Code/Ongoing projects/NLP_Disaster/dataset/sample_sub.csv\")\n",
    "\n",
    "# merging train and test datasets \n",
    "data=pd.concat([train, test], axis=0)\n",
    "data.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0     1.0  \n",
       "1     1.0  \n",
       "2     1.0  \n",
       "3     1.0  \n",
       "4     1.0  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 10876 entries, 0 to 3262\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   id        10876 non-null  int64  \n",
      " 1   keyword   10789 non-null  object \n",
      " 2   location  7238 non-null   object \n",
      " 3   text      10876 non-null  object \n",
      " 4   target    7613 non-null   float64\n",
      "dtypes: float64(1), int64(1), object(3)\n",
      "memory usage: 509.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0.0\n",
       "keyword       0\n",
       "location      0\n",
       "text          0\n",
       "target      0.0\n",
       "dtype: object"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.isna()].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    4342\n",
      "1.0    3271\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(data['target'].value_counts())\n",
    "data['target'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length of text when Disaster: 108.11342097217977\n",
      "Mean length of text when No Disaster: 95.70681713496084\n"
     ]
    }
   ],
   "source": [
    "# calculating the length of each document\n",
    "data['length'] = data['text'].apply(lambda x: len(x))\n",
    "print(\"Mean length of text when Disaster:\",data[data['target']==1].length.mean())\n",
    "print(\"Mean length of text when No Disaster:\",data[data['target']==0].length.mean())\n",
    "\n",
    "# Observation: longer tweet when actual disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of #words in tweeets from Disaster 1.9428238039673278\n",
      "Average number of #words in tweeets from No Disaster 1.9099201824401368\n"
     ]
    }
   ],
   "source": [
    "# #_word vs disaster\n",
    "\n",
    "# filter out the words that starts with #\n",
    "data[\"#word\"] = data['text'].apply(lambda x: list(word for word in x.split() if word[0]=='#'))\n",
    "\n",
    "# count # words in the text column\n",
    "data['number_of_#words']=data['#word'].apply(lambda x: len(x))\n",
    "\n",
    "# check averange numbr of #words in tweet with disaster and not disaster\n",
    "print(\"Average number of #words in tweeets from Disaster\",data[(data['target']==1) & (data['number_of_#words']!=0)]['number_of_#words'].mean())\n",
    "print(\"Average number of #words in tweeets from No Disaster\",data[(data['target']==0) & (data['number_of_#words']!=0)]['number_of_#words'].mean())\n",
    "\n",
    "# Observation: Not much of relation between number of # words used vs disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>#word</th>\n",
       "      <th>number_of_#words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  length                  #word  number_of_#words  \n",
       "0     1.0      69          [#earthquake]                 1  \n",
       "1     1.0      38                     []                 0  \n",
       "2     1.0     133                     []                 0  \n",
       "3     1.0      65           [#wildfires]                 1  \n",
       "4     1.0      88  [#Alaska, #wildfires]                 2  "
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# since no relation, removing #word and number_of_#words\n",
    "for col in ['#word', 'number_of_#words']:\n",
    "    if col in data.columns:\n",
    "        data.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>#word</th>\n",
       "      <th>number_of_#words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  length                  #word  number_of_#words  \n",
       "0     1.0      69          [#earthquake]                 1  \n",
       "1     1.0      38                     []                 0  \n",
       "2     1.0     133                     []                 0  \n",
       "3     1.0      65           [#wildfires]                 1  \n",
       "4     1.0      88  [#Alaska, #wildfires]                 2  "
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Punctuation Removal and Convert to Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case\n",
    "data['text']=data['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>#word</th>\n",
       "      <th>number_of_#words</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this earthquake ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>all residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>2</td>\n",
       "      <td>just got sent this photo from ruby alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this #earthquake m...   \n",
       "1   4     NaN      NaN             forest fire near la ronge sask. canada   \n",
       "2   5     NaN      NaN  all residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby #alaska as ...   \n",
       "\n",
       "   target  length                  #word  number_of_#words  \\\n",
       "0     1.0      69          [#earthquake]                 1   \n",
       "1     1.0      38                     []                 0   \n",
       "2     1.0     133                     []                 0   \n",
       "3     1.0      65           [#wildfires]                 1   \n",
       "4     1.0      88  [#Alaska, #wildfires]                 2   \n",
       "\n",
       "                                      processed_text  \n",
       "0  our deeds are the reason of this earthquake ma...  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  all residents asked to shelter in place are be...  \n",
       "3  13000 people receive wildfires evacuation orde...  \n",
       "4  just got sent this photo from ruby alaska as s...  "
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special character removeal\n",
    "def remove_special_characters(text):\n",
    "    # Define a list of special characters to remove\n",
    "    special_chars = \"!\\\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\n",
    "\n",
    "    # Remove special characters using str.translate() method\n",
    "    cleaned_text = text.translate(str.maketrans('', '', special_chars))\n",
    "    return cleaned_text\n",
    "\n",
    "data['processed_text']=data['text'].apply(lambda x: remove_special_characters(x))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tRemove Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>#word</th>\n",
       "      <th>number_of_#words</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>1</td>\n",
       "      <td>deeds reason earthquake may allah forgive us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>residents asked shelter place notified officer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>2</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this #earthquake m...   \n",
       "1   4     NaN      NaN             forest fire near la ronge sask. canada   \n",
       "2   5     NaN      NaN  all residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby #alaska as ...   \n",
       "\n",
       "   target  length                  #word  number_of_#words  \\\n",
       "0     1.0      69          [#earthquake]                 1   \n",
       "1     1.0      38                     []                 0   \n",
       "2     1.0     133                     []                 0   \n",
       "3     1.0      65           [#wildfires]                 1   \n",
       "4     1.0      88  [#Alaska, #wildfires]                 2   \n",
       "\n",
       "                                      processed_text  \n",
       "0       deeds reason earthquake may allah forgive us  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  residents asked shelter place notified officer...  \n",
       "3  13000 people receive wildfires evacuation orde...  \n",
       "4  got sent photo ruby alaska smoke wildfires pou...  "
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords\n",
    "stop= stopwords.words('english')\n",
    "\n",
    "data['processed_text'] = data['processed_text'].apply(lambda x: \" \".join([word for word in x.split() if word not in stop]))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>#word</th>\n",
       "      <th>number_of_#words</th>\n",
       "      <th>processed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69</td>\n",
       "      <td>[#earthquake]</td>\n",
       "      <td>1</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>38</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>133</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65</td>\n",
       "      <td>[#wildfires]</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfire evacuation order...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88</td>\n",
       "      <td>[#Alaska, #wildfires]</td>\n",
       "      <td>2</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this #earthquake m...   \n",
       "1   4     NaN      NaN             forest fire near la ronge sask. canada   \n",
       "2   5     NaN      NaN  all residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby #alaska as ...   \n",
       "\n",
       "   target  length                  #word  number_of_#words  \\\n",
       "0     1.0      69          [#earthquake]                 1   \n",
       "1     1.0      38                     []                 0   \n",
       "2     1.0     133                     []                 0   \n",
       "3     1.0      65           [#wildfires]                 1   \n",
       "4     1.0      88  [#Alaska, #wildfires]                 2   \n",
       "\n",
       "                                      processed_text  \n",
       "0         deed reason earthquake may allah forgive u  \n",
       "1              forest fire near la ronge sask canada  \n",
       "2  resident asked shelter place notified officer ...  \n",
       "3  13000 people receive wildfire evacuation order...  \n",
       "4  got sent photo ruby alaska smoke wildfire pour...  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "data['processed_text'].apply(lambda x: \" \".join([stem.stem(word) for word in x.split()]))\n",
    "\n",
    "# lemmatization\n",
    "lem = WordNetLemmatizer()\n",
    "data['processed_text'] = data['processed_text'].apply(lambda x: \" \".join([lem.lemmatize(word) for word in x.split()]))\n",
    "\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing URLs from the text\n",
    "def remove_urls(text):\n",
    "    pattern = r'http\\S+'  # Regex pattern to match URLs\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "data['processed_text']=data['processed_text'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to perform train & test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to original test and train\n",
    "\n",
    "i = train.shape[0]\n",
    "train = data[:i]\n",
    "test = data[i:]\n",
    "\n",
    "# x,y as the independent and dependent variable \n",
    "x=train[['processed_text','length']]\n",
    "y=train['target']\n",
    "test = test[['processed_text','length']]\n",
    "\n",
    "\n",
    "# spliting train and test data\n",
    "x_train,  x_val, y_train, y_val = train_test_split(x,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform to TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction using: TfidfVectorizer(max_features=3000)\n"
     ]
    }
   ],
   "source": [
    "# feature extraction \n",
    "\n",
    "def feature_extraction(fe):\n",
    "    print(\"Feature Extraction using:\",fe)\n",
    "    # transforming data\n",
    "    fe.fit(x_train['processed_text'])\n",
    "\n",
    "    x_train_fe = fe.transform(x_train['processed_text']).toarray()\n",
    "    x_val_fe = fe.transform(x_val['processed_text']).toarray()\n",
    "    test_fe = fe.transform(test['processed_text']).toarray()\n",
    "    \n",
    "    return(x_train_fe, x_val_fe, test_fe)\n",
    "\n",
    "\n",
    "max_feature = 3000\n",
    "cv = CountVectorizer(max_features=max_feature)\n",
    "tfidf = TfidfVectorizer(max_features=max_feature)\n",
    "\n",
    "x_train_fe, x_val_fe, test_fe = feature_extraction(tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform PCA (if required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Reduction: PCA(n_components=7)\n"
     ]
    }
   ],
   "source": [
    "def reduce_dimension(technique):\n",
    "    print(\"Feature Reduction:\", technique)\n",
    "    technique.fit(x_train_fe)\n",
    "    \n",
    "    x_train_fe_r = technique.transform(x_train_fe)\n",
    "    x_val_fe_r = technique.transform(x_val_fe)\n",
    "\n",
    "    test_fe_r = technique.transform(test_fe)\n",
    "\n",
    "    return( x_train_fe_r, x_val_fe_r, test_fe_r)\n",
    "\n",
    "\n",
    "\n",
    "n=7\n",
    "svd = TruncatedSVD(n_components=n)  # Specify the number of components to keep\n",
    "pca = PCA(n_components=n)\n",
    "\n",
    "x_train_fe_r, x_val_fe_r, test_fe_r = reduce_dimension(pca)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \tWrite a code to build Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report:(Train - LogisticRegression())\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.84      0.89      3872\n",
      "         1.0       0.77      0.91      0.83      2218\n",
      "\n",
      "    accuracy                           0.87      6090\n",
      "   macro avg       0.86      0.88      0.86      6090\n",
      "weighted avg       0.88      0.87      0.87      6090\n",
      "\n",
      "\n",
      " Classification Report: (Validation - LogisticRegression())\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.80      0.84       961\n",
      "         1.0       0.70      0.81      0.75       562\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.79      0.80      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# creating instance\n",
    "lr = LogisticRegression()\n",
    "# training model\n",
    "lr.fit(x_train_fe, y_train)\n",
    "# predicting for training data and validation set\n",
    "lr_pred_train = lr.predict(x_train_fe)\n",
    "lr_pred = lr.predict(x_val_fe)\n",
    "\n",
    "# report:\n",
    "print(f\"classification_report:(Train - {lr})\")\n",
    "print(classification_report(lr_pred_train,y_train))\n",
    "print(f\"\\n Classification Report: (Validation - {lr})\")\n",
    "print(classification_report(lr_pred, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a code to build Decision Tree Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report:(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.98      0.99      3545\n",
      "         1.0       0.97      1.00      0.98      2545\n",
      "\n",
      "    accuracy                           0.98      6090\n",
      "   macro avg       0.98      0.99      0.98      6090\n",
      "weighted avg       0.98      0.98      0.98      6090\n",
      "\n",
      "\n",
      " Classification Report: (Validation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.75      0.74       858\n",
      "         1.0       0.67      0.65      0.66       665\n",
      "\n",
      "    accuracy                           0.71      1523\n",
      "   macro avg       0.70      0.70      0.70      1523\n",
      "weighted avg       0.71      0.71      0.71      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# creating instance\n",
    "dtc = DecisionTreeClassifier()\n",
    "# training model\n",
    "dtc.fit(x_train_fe, y_train)\n",
    "# predicting for training data and validation set\n",
    "dtc_pred_train = dtc.predict(x_train_fe)\n",
    "dtc_pred = dtc.predict(x_val_fe)\n",
    "\n",
    "# report:\n",
    "print(\"classification_report:(Train)\")\n",
    "print(classification_report(dtc_pred_train,y_train))\n",
    "print(\"\\n Classification Report: (Validation)\")\n",
    "print(classification_report(dtc_pred, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a code to build Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report:(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.98      0.99      3521\n",
      "         1.0       0.97      0.99      0.98      2569\n",
      "\n",
      "    accuracy                           0.98      6090\n",
      "   macro avg       0.98      0.99      0.98      6090\n",
      "weighted avg       0.98      0.98      0.98      6090\n",
      "\n",
      "\n",
      " Classification Report: (Validation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.79      0.81       910\n",
      "         1.0       0.71      0.75      0.73       613\n",
      "\n",
      "    accuracy                           0.78      1523\n",
      "   macro avg       0.77      0.77      0.77      1523\n",
      "weighted avg       0.78      0.78      0.78      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# creating instance\n",
    "rfc = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "# training model\n",
    "rfc.fit(x_train_fe, y_train)\n",
    "# predicting for training data and validation set\n",
    "rfc_pred_train = rfc.predict(x_train_fe)\n",
    "rfc_pred = rfc.predict(x_val_fe)\n",
    "\n",
    "# report:\n",
    "print(\"classification_report:(Train)\")\n",
    "print(classification_report(rfc_pred_train,y_train))\n",
    "print(\"\\n Classification Report: (Validation)\")\n",
    "print(classification_report(rfc_pred, y_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the best model and build model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report:(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.99      0.93      0.96      3681\n",
      "         1.0       0.90      0.98      0.94      2409\n",
      "\n",
      "    accuracy                           0.95      6090\n",
      "   macro avg       0.94      0.95      0.95      6090\n",
      "weighted avg       0.95      0.95      0.95      6090\n",
      "\n",
      "\n",
      " Classification Report: (Validation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.79      0.84       980\n",
      "         1.0       0.69      0.82      0.75       543\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.79      0.81      0.79      1523\n",
      "weighted avg       0.82      0.80      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# creating instance\n",
    "svm = svm.SVC()\n",
    "# training model\n",
    "svm.fit(x_train_fe, y_train)\n",
    "# predicting for training data and validation set\n",
    "svm_pred_train = svm.predict(x_train_fe)\n",
    "svm_pred = svm.predict(x_val_fe)\n",
    "\n",
    "# report:\n",
    "print(\"classification_report:(Train)\")\n",
    "print(classification_report(svm_pred_train,y_train))\n",
    "print(\"\\n Classification Report: (Validation)\")\n",
    "print(classification_report(svm_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification_report:(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.83      0.88      3904\n",
      "         1.0       0.75      0.90      0.81      2186\n",
      "\n",
      "    accuracy                           0.85      6090\n",
      "   macro avg       0.84      0.86      0.85      6090\n",
      "weighted avg       0.87      0.85      0.86      6090\n",
      "\n",
      "\n",
      " Classification Report: (Validation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.79      0.83       972\n",
      "         1.0       0.68      0.80      0.73       551\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.79      0.78      1523\n",
      "weighted avg       0.80      0.79      0.79      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# creating instance\n",
    "mnb = MultinomialNB()\n",
    "# training model\n",
    "mnb.fit(x_train_fe, y_train)\n",
    "# predicting for training data and validation set\n",
    "mnb_pred_train = mnb.predict(x_train_fe)\n",
    "mnb_pred = mnb.predict(x_val_fe)\n",
    "\n",
    "# report:\n",
    "print(\"classification_report:(Train)\")\n",
    "print(classification_report(mnb_pred_train,y_train))\n",
    "print(\"\\n Classification Report: (Validation)\")\n",
    "print(classification_report(mnb_pred, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(model, \n",
    "                x_train= x_train_fe,\n",
    "                x_val=x_val_fe,\n",
    "                x_test=test_fe):\n",
    "    print(\"Model used:\",model)\n",
    "    # training model\n",
    "    model.fit(x_train, y_train)\n",
    "    # predicting for training data and validation set\n",
    "    model_pred_train = model.predict(x_train)\n",
    "    model_pred = model.predict(x_val)\n",
    "    test_pred = model.predict(x_test)\n",
    "\n",
    "    # report:\n",
    "    print(\"classification_report:(Train)\")\n",
    "    print(classification_report(model_pred_train,y_train))\n",
    "    print(\"Accuracy Score:Training\",accuracy_score(model_pred_train,y_train))\n",
    "    print(\"\\n Classification Report: (Validation)\")\n",
    "    print(classification_report(model_pred, y_val))\n",
    "    print(\"Accuracy Score:\",accuracy_score(model_pred, y_val))\n",
    "\n",
    "    return (test_pred)\n",
    "\n",
    "# model train and prediction\n",
    "instance_lr = LogisticRegression()\n",
    "instance_dtc = DecisionTreeClassifier()\n",
    "instance_rfc = RandomForestClassifier(n_estimators=500, random_state=42)\n",
    "#instance_svm = svm.SVC()\n",
    "instance_mnb = MultinomialNB()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction using: TfidfVectorizer(max_features=3000)\n",
      "Feature Reduction: PCA(n_components=7)\n",
      "Building model without frature reduction:\n",
      "Model used: LogisticRegression()\n",
      "classification_report:(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.84      0.89      3872\n",
      "         1.0       0.77      0.91      0.83      2218\n",
      "\n",
      "    accuracy                           0.87      6090\n",
      "   macro avg       0.86      0.88      0.86      6090\n",
      "weighted avg       0.88      0.87      0.87      6090\n",
      "\n",
      "Accuracy Score:Training 0.8679802955665025\n",
      "\n",
      " Classification Report: (Validation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      0.80      0.84       961\n",
      "         1.0       0.70      0.81      0.75       562\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.79      0.80      0.79      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "Accuracy Score: 0.8023637557452397\n",
      "\n",
      "Building model after reducing features:\n",
      "Model used: LogisticRegression()\n",
      "classification_report:(Train)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.65      0.77      4951\n",
      "         1.0       0.35      0.80      0.49      1139\n",
      "\n",
      "    accuracy                           0.68      6090\n",
      "   macro avg       0.64      0.73      0.63      6090\n",
      "weighted avg       0.83      0.68      0.72      6090\n",
      "\n",
      "Accuracy Score:Training 0.6822660098522167\n",
      "\n",
      " Classification Report: (Validation)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.66      0.77      1245\n",
      "         1.0       0.35      0.81      0.48       278\n",
      "\n",
      "    accuracy                           0.69      1523\n",
      "   macro avg       0.64      0.73      0.63      1523\n",
      "weighted avg       0.83      0.69      0.72      1523\n",
      "\n",
      "Accuracy Score: 0.685489166119501\n"
     ]
    }
   ],
   "source": [
    "# Features extraction\n",
    "maxf = 3000\n",
    "cv = CountVectorizer(max_features=maxf)\n",
    "tfidf = TfidfVectorizer(max_features=maxf)\n",
    "x_train_fe, x_val_fe, test_fe = feature_extraction(tfidf)\n",
    "\n",
    "\n",
    "# features reduction\n",
    "n=7\n",
    "svd = TruncatedSVD(n_components=n)  # Specify the number of components to keep\n",
    "pca = PCA(n_components=n)\n",
    "x_train_fe_r, x_val_fe_r, test_fe_r = reduce_dimension(pca)\n",
    "\n",
    "\n",
    "print(\"Building model without frature reduction:\")\n",
    "sub_test_pred = build_model(instance_lr, \n",
    "                        x_train= x_train_fe, \n",
    "                        x_val=x_val_fe, \n",
    "                        x_test=test_fe)\n",
    "\n",
    "print(\"\\nBuilding model after reducing features:\")\n",
    "test_pred = build_model(instance_lr, \n",
    "                        x_train= x_train_fe_r, \n",
    "                        x_val=x_val_fe_r, \n",
    "                        x_test=test_fe_r)\n",
    "\n",
    "\n",
    "# F1 score is lower if dimensionality reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ankitbaliyan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/ankitbaliyan/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/ankitbaliyan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n",
      "/Users/ankitbaliyan/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:1688: FutureWarning: Feature names only support names that are all strings. Got feature names with dtypes: ['int', 'str']. An error will be raised in 1.2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8069599474720945\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.89      0.84       874\n",
      "         1.0       0.82      0.70      0.76       649\n",
      "\n",
      "    accuracy                           0.81      1523\n",
      "   macro avg       0.81      0.79      0.80      1523\n",
      "weighted avg       0.81      0.81      0.80      1523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split test and train\n",
    "\n",
    "i = train.shape[0]\n",
    "train = data[:i]\n",
    "test = data[i:]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into features (text and length) and target\n",
    "X = train[['processed_text', 'length','number_of_#words']]\n",
    "y = train['target']\n",
    "test = test[['processed_text','length','number_of_#words']]\n",
    "\n",
    "# Convert text to numerical representation using TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=3000)\n",
    "vectorizer.fit(X['processed_text'])\n",
    "X_text = vectorizer.transform(X['processed_text'])\n",
    "test_text = vectorizer.transform(test['processed_text'])\n",
    "\n",
    "# Combine text and length features\n",
    "X_combined = pd.concat([pd.DataFrame(X_text.toarray()), X[['length','number_of_#words']]], axis=1)\n",
    "test_combined = pd.concat([pd.DataFrame(test_text.toarray()), test[['length','number_of_#words']]], axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train the logistic regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "submission['target']=model.predict(test_combined)\n",
    "\n",
    "# Calculate and print the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2085\n",
      "1    1178\n",
      "Name: target, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       1\n",
       "1   2       1\n",
       "2   3       1\n",
       "3   9       1\n",
       "4  11       1"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['target'] = submission['target'].apply(lambda x: round(x))\n",
    "#submission.to_csv(\"/Users/ankitbaliyan/Documents/VS_Code/Ongoing projects/NLP_Disaster/subission file/submission_9.csv\", index=False)\n",
    "print(submission.target.value_counts())\n",
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
